---  
output:html_document  
toc:true  
keep_md: yes  
---
```{r, echo=FALSE,results='hide'}
##ProjCodeReport_July20_1141am version
```
**Coursera Practical Machine Learning Project July 2015:   
Machine Learning Model From Weight Lifting Exercises Dataset**  
Ram Krishnan (May 19, 2015)  
**Executive Summary**  
This dataset has been analyzed by many researchers (Refs 2-3) who have all concluded that Random Forest is the best algorithm and is accurate to >99%! One researcher has found that other methods (Ref 2) are significantly less accurate. We will use the Random Forest method based on those earlier findings but extend the results to answer the following question:   
*How can the results be used to build a practical appliance that can provide real-time feedback to the weight-lifter?*   
The appliance must be low-cost (e.g. the FitBit), thus have minimum hardware i.e. the smallest number of sensors. The original dataset has 52 variables i.e. 52 sensors that provide feedback. That is clearly too many for a practical appliance that could be sold for consumer electronics type prices. Additionally, fewer sensors will result in shorter processing time for the predictor model which is necessary for real-time feedback to the weight-lifter. To determine which sensors are important and must be left in a practical device, it also helps to have a more interpretable model such as Random Forests as compared to Principal Components where the outputs from many sensors could get merged into a single feedback signal. To accomplish the objective of fewest sensors, we determined that only 7 variables were most important using the Importance matrix component of the Random Forest model. We then examined the accuracy of a predictor using only these 7 variables. The full model (52 variables) has a testing accuracy of 99.5% and the reduced, 7-variable model has an accuracy of 98.7% which is quite acceptable especially for a significantly lower cost and complexity. As a bonus, the reduced (7-variable) predictor requires 0.318 secs for real-time predictions compared to the full (52-variable) preditor which requires 0.405 secs, a 21% reduction in prediction time. The 7-sensor model was also tested against the 20 observation testing dataset and the predictions match those of other researchers.
```{r, echo=FALSE,results='hide',cache=TRUE}
library(lattice)
library(ggplot2)
library(randomForest)
library(downloader)
library(caret)
library(AppliedPredictiveModeling)
download("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile="training.csv")
download("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile="testing.csv")
```
**Exploratory Data Analysis**  
First, we pre-process the training data to remove all irrelevant data (first 7 columns), columns containing all zeros and columns containing factor variables (which vary very little and therefore contribute nothing to the model). Next, we add the output (factor) variable "classe" back in.  
```{r, echo=FALSE,results='hide',cache=TRUE}
training <- read.csv("training.csv")
testing <- read.csv("testing.csv")
training1 <- training[,-c(1:7)]
testing1 <- testing[,-c(1:7)]
```
```{r, echo=FALSE,results='hide',cache=FALSE}
colClass <- sapply(training1, class)
colF <- which(colClass=="factor")
training2 <- training1[, -colF]
testing2 <- testing1[, -colF]
colSums.NA <- colSums(is.na(training2))
training3 <- training2[, which(colSums.NA==0)]
training.Processed <- cbind(training3, classe = training$classe)
testing3 <- testing2[, which(colSums.NA==0)]
testing.Processed <- cbind(testing3, classe = testing$problem_id)
```
Observing that the testing data does not have the classe variable, hence cannot be used for validation, we create a separate validation set by splitting the training set into 75% for training and 25% for validation.   
**Modeling and Prediction**  
We build a model using the randomForest() command set rather than the train() command set because it was found to process in <5 minutes compared to train("rf") which took 1.23 hours! Of course, train("rf") could probably be speeded up by changing the default values but since randomForest() results in great accuracy (>99%), it was the default command set. Next we use the created validation test set to validate the result with the confusion matrix and the test accuracy. 
```{r, echo=FALSE,cache=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(123)
useData <- createDataPartition(y=training.Processed$classe, p=0.75, list=FALSE)
myTraining <- training.Processed[useData,]
myTesting <- training.Processed[-useData,]
library(randomForest)
set.seed(1234)
modFit.Full_RF <- randomForest(classe~., data=myTraining, importance = TRUE, proximity = TRUE)
t1 <- Sys.time()
pred.RF <- predict(modFit.Full_RF, myTesting)
t2 <- Sys.time()
Full_RF.PredictTime <- difftime(t2,t1)
confusion.Full_RF <- confusionMatrix(pred.RF, myTesting$classe)
print(modFit.Full_RF$call)
##print(modFit.Full_RF)
print(confusion.Full_RF$overall)
cat("Full RF model prediction time:",Full_RF.PredictTime)
```
The randomForest command conveniently provides an Importance matrix which lists the variables in order of importance based on the mean decrease in the Gini index. We sort the variables according to importance and keep the top 7 (mean decrease in Gini scores beyond var 8 are relatively low):
```{r, echo=FALSE,cache=FALSE}
ImpVarTemp <- sort(modFit.Full_RF$importance[,7],decreasing = TRUE)
ImpVar <- ImpVarTemp[1:7]
training.Imp <- myTraining[,names(ImpVar)]
testing.Imp <- myTesting[,names(ImpVar)]
cat("7 Most Imp Var:",names(ImpVar),",")
```
The most important 7 variables are combined with the "classe" output variable to form a new training set and testing set. A model is created using the randomForest command again.
```{r, echo=FALSE,results='hide',cache=FALSE}
modFit.Imp <- randomForest(myTraining$classe~., data=training.Imp, importance = TRUE, proximity = TRUE)
```
The reduced test set is used to check the prediction accuracy of the model.
```{r, echo=FALSE,cache=FALSE}
t3 <- Sys.time()
Imp_RF.predictor <- predict(modFit.Imp,testing.Imp)
t4 <- Sys.time()
ImpVar.PredictTime <- difftime(t4,t3)
cat("Top 7 Var RF model prediction time:",ImpVar.PredictTime)
confMat.Imp <- confusionMatrix(Imp_RF.predictor,myTesting$classe)
print(confMat.Imp$overall)
```
Finally, we plot the reduced var set confusion matrix to visually check the accuracy of each output level and see that the levels are accurately centered on the actuals and >98% accurate.  
```{r, echo=FALSE,cache=TRUE}
##Plot random forest confusion matrix
library(som)
library(reshape2)
table2 <- confMat.Imp$table
table2.normalized <- normalize(table2)
colnames(table2.normalized) <- rownames(table2.normalized)
table2.melt <- melt(table2.normalized)
names(table2.melt) <- c("Prediction","Reference","N.Frequency")
plot2 <- ggplot(table2.melt)
plot3 <-plot2 + geom_tile(aes(x=Reference, y=Prediction, fill=N.Frequency)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") +labs(title = "Normalized Confusion Matrix, Random Forest w/ Top 7 Var")
plot3
```{r, echo=FALSE,cache=FALSE}
##Predict with test data and check accuracy
predictionFinal <- predict(modFit.Imp, testing.Processed)
print("Test set Prediction Using Top 7 Variables RF Model:")
print(predictionFinal)
```  
**References:**  
1. "An Introduction to Statistical Learning", James, Witten, Hastie, Tibshirani, Springer 
2. "Machine Learning for Weight Lifting Exercises Data", Shiming Zhou, http://shinezhou9.github.io/MachineLearningProject1/
3. "Practical Machine Learning: Peer Assesment", Rithesh Kumar, http://www.rpubs.com/ritheshkumar95/35118

